# Neurosymbolic Knowledge Graph Builder

This module (**finalKG**) is responsible for constructing the foundational Knowledge Graph (KG) used by the Neurosymbolic Drug Repurposing platform. It manages the **Extract-Transform-Load (ETL)** pipeline, converting raw biomedical data (Hetionet) into a machine-learning-ready format for the R-GCN model.

---

## üèóÔ∏è Pipeline Overview

The construction of the Knowledge Graph follows a strict sequence of operations:

1.  **Ingestion** (`hetionet_to_neo4j.py`): Downloads Hetionet v1.0 and loads it into a Neo4j database.
2.  **Extraction** (`hetionet_extract.py`): Queries Neo4j to export specific subgraphs (nodes/edges) relevant to drug repurposing into CSV format.
3.  **Preprocessing** (`preprocess.py`): Cleans the CSV data, maps entity IDs to continuous integers, and generates triples.
4.  **Construction** (`build_graph.py`): Converts processed data into a PyTorch Geometric (`HeteroData`) object.
5.  **Embeddings** (`add_embeddings.py`): Initializes node features/embeddings for the graph.
6.  **Indexing** (`build_polo_index.py`): Creates a NetworkX index for symbolic graph algorithms.

---

## ‚öôÔ∏è Configuration

Before running any scripts, you must configure the environment variables.

1.  Copy the example environment file:
    ```bash
    cp .env.example .env
    ```
2.  Edit `.env` and set your Neo4j credentials and paths:
    ```ini
    NEO4J_URI=bolt://localhost:7687
    NEO4J_USER=neo4j
    NEO4J_PASS=your_secure_password
    DATA_DIR=./data
    ```

---

## üì¶ Data Setup (Critical)

> [!IMPORTANT]
> The large data files required for this graph are **NOT** stored in this repository due to size limits. You must download or regenerate them.

### Option 1: Regenerate from Source (Recommended)
You can rebuild the entire graph from scratch using the scripts provided.

1.  **Install Requirements**:
    ```bash
    pip install -r requirements.txt
    ```
2.  **Setup Neo4j**: Ensure a local Neo4j instance is running and configured.
3.  **Run Pipeline**:
    ```bash
    # 1. Download & Load (This checks for hetionet-v1.0.json.bz2 automatically)
    python scripts/hetionet_to_neo4j.py

    # 2. Export to CSV (creates data/raw_nodes.csv, data/raw_edges.csv)
    python scripts/hetionet_extract.py

    # 3. Clean & Map (creates data/nodes.csv, data/relations.csv, data/triples.txt)
    python scripts/preprocess.py

    # 4. Build PyTorch Graph (creates data/graph.pt)
    python scripts/build_graph.py

    # 5. Add Embeddings (creates data/graph_with_embeddings.pt)
    python scripts/add_embeddings.py

    # 6. Build Symbolic Index (creates data/graph_index.pkl for Polo Agent)
    python scripts/build_polo_index.py
    ```

### Option 2: Resume from Backup
If you have been provided with a data backup:
1.  Place `hetionet-v1.0.json.bz2` inside `finalKG/data/`.
2.  Place `graph.pt`, `graph_index.pkl`, `nodes.csv`, and `compound_disease_predictions.npy` in `finalKG/data/` (or `backend/` where required).

---

## üìÇ Directory Structure

-   **`data/`**: Stores raw and processed artifacts.
    -   `raw_nodes.csv`, `raw_edges.csv`: Exports from Neo4j.
    -   `nodes.csv`, `relations.csv`: Cleaned, integer-mapped files.
    -   `graph.pt`: The initial PyTorch Geometric graph object (Binary).
    -   `graph_with_embeddings.pt`: The finalized graph with node embeddings (Used by R-GCN).
    -   `graph_index.pkl`: NetworkX graph index for the Polo Agent (Symbolic Logic).
-   **`scripts/`**: Python ETL scripts (see Overview).
-   **`requirements.txt`**: Dependencies for the graph builder (Neo4j driver, PyTorch, Pandas).

## üß† Technical Artifacts explained

Understanding the two binary files generated by this pipeline:

### 1. `graph_with_embeddings.pt` (The "Model Input")
*   **What is it?**: A PyTorch Geometric `HeteroData` object. It contains the graph structure (Nodes, Edges) and **Node Embeddings** converted into Tensors.
*   **Generated By**: `scripts/add_embeddings.py` (which transforms `graph.pt`)
*   **Why?**: The R-GCN Neural Network cannot read CSV files directly. It needs optimized tensors for matrix multiplication and message passing.
*   **Logic**:
    1.  Reads `nodes.csv` and `relations.csv` (via `build_graph.py` to create `graph.pt`).
    2.  Converts entity IDs to integers (0 to N).
    3.  Initializes or appends node feature matrices (`x` attributes).
    4.  Saves as a compressed binary `.pt` file.

### 2. `graph_index.pkl` (The "Agent Index")
*   **What is it?**: A serialized Python `NetworkX` graph object.
*   **Generated By**: `scripts/build_polo_index.py`
*   **Why?**: The Symbolic Agent ("Polo") needs to perform graph search algorithms (Shortest Path) which are difficult on raw tensors. `pickle` allows instant loading of the pre-built graph structure into memory, bypassing the 3-minute CSV parsing time.
*   **Logic**:
    1.  Reads processed CSVs.
    2.  Builds a standard Python graph in memory (`G.add_edge(...)`).
    3.  Freezes the object state to `graph_index.pkl`.

---

## üõ†Ô∏è Verification

To verify that the graph built correctly, run the validation script:

```bash
python scripts/validate_rgcn_readiness.py
```
This will check for data consistency, node type correctness, and file integrity.
